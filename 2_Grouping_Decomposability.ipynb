{
 "metadata": {
  "name": "",
  "signature": "sha256:1b381a27b33b3b1245005098272efabf7117d513cafd9e508a933eaf968fb8a0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Grouping Decomposability"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider an ordered list\n",
      "$$x_1 < x_2 < ...< x_N$$\n",
      "The shortest way to answer the question, 'Does the list contain $x$?', is the binary search.  Some things to note:\n",
      "\n",
      "* The binary search requires $\\log_2 N$ queries and this happens to be the entropy.\n",
      "* The binary search could be understood as 'asking the most uncertain question', ie dividing the possible outcomes as evenly as possible between the possible answers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Grouping Decomposability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can calculate the entropy by asking questions. Consider the set $x = \\{a, b, c\\}$ with probabilities $\\{\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\}$.  The entropy of this distribution is $H = \\frac{3}{2} bits$.  We could also decompose it into a set of questions:\n",
      "* Is the unknown $a$ or $\\{b, c\\}$ with probabilites $\\{\\frac{1}{2}, \\frac{1}{2}\\}$\n",
      "* And in the second case, $b$ or $c$ with probabilties $\\{\\frac{1}{2}, \\frac{1}{2}\\}$.  As this case occurs with probability $\\frac{1}{2}$, we write\n",
      "$$H(\\{\\frac{1}{2}, \\frac{1}{2}\\}) + \\frac{1}{2} H(\\{\\frac{1}{2}, \\frac{1}{2}\\}) = \\frac{3}{2} = H$$\n",
      "\n",
      "Generalizing this property, we write,\n",
      "$$H(p_1...p_I) = H(p_1+...+p_m, p_{m+1}+...+p_I) + (p_1+...+p_m) H(\\frac{p_1}{p_1+...+p_m}, ..., \\frac{p_m}{p_1+...+p_m}) + (p_{m+1}+...+p_I) H(\\frac{p_{m+1}}{p_{m+1}+...+p_I},...,\\frac{p_{I}}{p_{m+1}+...+p_I})$$\n",
      "This is the *grouping decomposability property* and it is the fundamental property of the Shannon entropy."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Axiomatic Derivation of the Entropy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Theorem** for a random variable $X = \\{p_1, ...,p_I\\}$, the three properties,\n",
      "\n",
      "* $H$ is continuous in $p_i$.\n",
      "* if $p_i = \\frac{1}{I}$, $H$ is monotonically increasing in $I$.\n",
      "* Grouping Decomposability\n",
      "\n",
      "determine the functional form of $H$ to be $H(\\{p\\}) = -C\\sum_{i} p_i \\log p_i$ where $C$ is an arbitrary positive constant.\n",
      "\n",
      "**Proof**  The proof is sketched out as follows:\n",
      "\n",
      "* Consider $I=k^m$ equiprobable events and use property 3 to show $H(\\{\\frac{1}{k^m}\\}) = m H(\\{\\frac{1}{k}\\})$\n",
      "* For equiprobable events $k^m \\le l^m \\le k^{m+1}$ use property 2, $H(\\{\\frac{1}{k^m}\\}) \\le H(\\{\\frac{1}{l^m}\\}) \\le H(\\{\\frac{1}{k^{m+1}}\\})$ to conclude $\\left|\\frac{H(\\{\\frac{1}{l}\\})}{H(\\{\\frac{1}{k}\\})}  - \\frac{\\log l}{\\log k}\\right| < 2\\epsilon$ and thus $H(\\{\\frac{1}{l}\\}) \\propto \\log l$\n",
      "* Consider a set of $K$ equiprobable events in $I$ groups each containing $k_i$.  Define $p_i = \\frac{k_i}{K}$.  By grouping decomposability $H(\\{\\frac{1}{K}\\}) = H(p_1,...,p_I) + \\sum_{i=1}^I p_i H(\\{\\frac{1}{k_i}\\})$ giving $H(p_1,...,p_I)$.\n",
      "* Use continuity to extend to irrationals."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}