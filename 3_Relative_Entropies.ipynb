{
 "metadata": {
  "name": "",
  "signature": "sha256:db3ad324b05fa0014cc2b4babd33c44a78bf006cbdcb1c7923fd42385b492ff5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Relative Entropy"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kullback-Liebler Difference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given two distributions, $\\{p_i\\}$ and $\\{q_i\\}$, we want a measure of the difference between the two.\n",
      "\n",
      "We define the **Kullback-Liebler Divergence** as\n",
      "$$D(p||q)\\equiv \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} = \\langle \\log_2 \\frac{p_i}{q_i} \\rangle_p$$\n",
      "Notice,\n",
      "\n",
      "* If one $p_i = 0$, this term gives $0\\,\\log_2 \\frac{0}{q_i} = 0$ so this gives no contribution\n",
      "* If a $q_i = 0$, then $p_i\\,\\log_2 \\frac{p_i}{0} = \\infty$ if $p_i=0$\n",
      "* While positive, it is not symmetric and does not satisfy the triangle inequality and is thus *not a distance*\n",
      "\n",
      "**Example**\n",
      "$$p(0) = r, \\; p(1) = 1-r$$\n",
      "$$q(0) = s, \\; q(1) = 1-s$$\n",
      "For $r = \\frac{1}{2}$ and $s = \\frac{1}{4}$,\n",
      "$$D(p||q) = r\\,\\log_2\\frac{r}{s} + (1-r)\\,\\log_2\\frac{1-r}{1-s} = 0.2075 \\;bits$$\n",
      "$$D(q||p) = s\\,\\log_2\\frac{s}{r} + (1-s)\\,\\log_2\\frac{1-s}{1-r} = 0.1887 \\;bits$$\n",
      "If $r=s$\n",
      "$$D(p||q) = D(q||p) = 0$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* If $p=q$, $D(p||q) = D(q||p) = 0$\n",
      "* $D(p||q)$ is non-negative:  \n",
      "$$\\ln 2 \\,D(p||q) = -\\sum p_i \\ln \\frac{q_i}{p_i} \\ge -\\sum p_i (\\frac{q_i}{p_i} - 1) = 0$$\n",
      "\n",
      "* Can use to prove $H(x) \\le \\log_2 I$  \n",
      "$$D(p||\\{\\frac{1}{I}\\}) = \\sum p_i \\log_2 p_i - \\sum p_i \\log_2 \\frac{1}{I} = -H(x) + \\log_2 I \\ge 0$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Aside - Defining a true distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An obvious first step is to consider the symmetrized KL divergence:\n",
      "$$\\tilde{D}(p,q) = \\frac{1}{2}\\left[ D(p||q) + D(q||p)\\right].$$\n",
      "This quantity is symmetric but does not satisfy the triangle inequality. The Jensen-Shannon distance does.  Defining $M = \\frac{1}{2}(p+q)$, a true distance is,\n",
      "$$JS(p,q) = \\sqrt{\\frac{1}{2}\\left[ D(p||M) +  D(q||M)\\right]}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mutual Information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For random variables $X$ and $Y$, how much does knowing $X$ tell you about $Y$?  For joint probability $p_{ij}\\equiv$ prob. that $x=i$ and $y=j$, we define the mutual information,\n",
      "$$I(X,Y) = \\sum_{ij} p_{ij}\\log_2 \\frac{p_{ij}}{p_i q_j} = D(p_{ij}||p_i q_j)$$\n",
      "where $p_i = \\sum_j p_{ij}$ and $q_j = \\sum_i p_{ij}$.\n",
      "\n",
      "* A bit of algebra shows\n",
      "$$I(X,Y) = H(X) - H(X|Y)$$\n",
      "so the mutual information is the total uncertainty in $X$ minus the remaining uncertainty after we learn $Y$, ie it's how much we learn about $X$ upon learning $Y$.\n",
      "\n",
      "* As $I(X,Y)$ is a KL difference, we immediately get,\n",
      "$$H(X) \\le H(X|Y)$$\n",
      "so conditioning always lowers the entropy *on average*.\n",
      "\n",
      "* If $X$ and $Y$ are independent,\n",
      "$$p_{ij} = p_i q_j \\to I(X,Y) = 0$$\n",
      "\n",
      "* If $Y = g(X)$ where $g$ is a deterministic function, $I(X,Y) = H(Y)$.\n",
      "\n",
      "**Example**\n",
      "$$p_{11} = 0, \\; p_{12} = \\frac{1}{8},\\; p_{21} = \\frac{3}{4},\\; p_{22} = \\frac{1}{8}$$\n",
      "These give  marginals and conditionals, \n",
      "$$p(x) = \\{\\frac{1}{8}, \\frac{7}{8}\\}, \\; p(x|y=1) = \\{0, 1\\}, \\; p(x|y=2) = \\{\\frac{1}{2}, \\frac{1}{2}\\}$$\n",
      "and entropy and conditional entropy,\n",
      "$$ H(X) = 0.544 \\,\\text{bits} \\\\\n",
      "H(X|Y=1) = 0 \\,\\text{bits} \\\\\n",
      "H(X|Y=2) = 1 \\,\\text{bit}$$\n",
      "So while an individual conditional entropy may be greater or lesser than the entropy of the variable, on average it will decrease."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}