{
 "metadata": {
  "name": "",
  "signature": "sha256:8c0f151dae9ff6032b24da8ed3a120000630714ab3e6c3b47527bfec4d33f7de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction to Entropy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition** The Shannon entropy for a discrete set of events with probabilities $\\{p_1, ...,p_I\\}$ is,\n",
      "$$H(\\{p_i\\}) = -\\sum_{i+1}^I p_i \\log_2 p_i$$\n",
      "\n",
      "Some notes:\n",
      "\n",
      "* The base of the $\\log$ is arbitrary and it may be convenient to change it.  In base 2, the measurement units is bits, in base $e$ nats.\n",
      "* It satisfies $0\\le H(\\{p_i\\}) \\le \\log_2 I$\n",
      "* $H$ increases with the 'breadth' of a distribution and is at a max when all events are equiprobable.  It is thus considered as a measure of the 'uncertainty' in a random variable.\n",
      "* It is additive for independent variables, $H(X, Y) = H(X) + H(Y)$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Renyi Entropies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternative definition of entropy that is still a measure of uncertainty.  It lacks grouping decomposability and extensivity for non-independent RV except when $\\alpha = 1$\n",
      "$$H_\\alpha(\\{p_i\\}) = \\frac{1}{1-\\alpha}\\log_2\\left(\\sum_{i=1}^I p_i^\\alpha\\right)$$\n",
      "You can show that,\n",
      "\n",
      "* as $\\alpha\\to 1$ we obtain the Shannon entropy (by L'hopital)\n",
      "* as $\\alpha\\to\\infty$ $H_\\alpha \\to -\\log_2 p_{max}$ (use $p_{max}^\\alpha \\le \\sum p_i^\\alpha \\le Np_{max}^\\alpha$ and squeeze)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Joint Entropy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the same definition for entropy on the joint distribution and this gives a decomposition\n",
      "\\begin{align}\n",
      "H(X, Y) &= -\\sum_{ij}p_{ij} \\log_2 p_{ij} = -\\sum_i p_i \\log_2 p_i - \\sum_i p_i \\sum_j p_{j|i} \\log_2 p_{j|i}\\\\\n",
      "H(X, Y) &= H(X) + H(Y|X) = H(Y) + H(X|Y)\n",
      "\\end{align}\n",
      "where the **conditional entropy** is $H(Y|X) = - \\sum_i p_i \\sum_j p_{j|i} \\log_2 p_{j|i}$ which we can think of as the average uncertainty in $X$ after learning $Y$.\n",
      "\n",
      "This gives a decomposition mirroring $p(a, b, c) = p(a)\\,p(b|a)\\,p(c|b,a)$,\n",
      "$$H(x_1,...x_n) = H(x_1) + H(x_2|x_1) + ...+ H(x_n|...,x_1)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}