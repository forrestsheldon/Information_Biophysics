{
 "metadata": {
  "name": "",
  "signature": "sha256:df32cb298e0a3ff0c01a5aeb362d9700f3e8b70c2d42f4e4cc9532ae8ff9084d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Estimating Entropies from Real Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as scp\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Information on this can be found in\n",
      "* Bialek's Biophysics book Appendix A.8\n",
      "* [Strong et. al. PRL](http://journals.aps.org/prl/pdf/10.1103/PhysRevLett.80.197)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Systematic Bias in Binning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The obvious/naive way of estimating the entropy is to use the frequencies of events to approximate the probabilities.  In a large number of samples $N$ we expect $f_i = \\frac{n_i}{N} \\to p_i$. There is a problem however: in any finite sample, we expect rare events to be undersampled.  This will tend to concentrate the probability into a more narrow range of events and thus reduce the entropy. Estimates of the entropy should thus be systematically biased towards lower values.  Let's try to show it: \n",
      "\n",
      "We can write $f_i = p_i + \\delta f_i$ and by evaluating the moments of a binomial RV we expect $\\langle\\delta f_i\\rangle = 0$ and $\\langle\\delta f_i^2\\rangle = \\frac{p_i(1-p_i)}{N}$.  Our entropy estimate is,\n",
      "$$H_{est} = - \\sum_{i=1}^K f_i \\log_2 f_i = -\\sum_{i=1}^K (p_i + \\delta f_i) \\, \\log_2 (p_i + \\delta f_i) $$\n",
      "Keeping terms of order $ \\delta f^2$ we arrive at\n",
      "$$\\ln 2 H_{est} = -\\sum_{i=1}^K p_i \\ln p_i -  \\sum_{i=1}^K  \\delta f_i ( 1 + \\ln p_i) - \\sum_{i=1}^K \\frac{\\delta f_i^2}{2p_i} $$\n",
      "Now, averaging over an ensemble of samples, we have,\n",
      "$$\\ln 2 \\langle H_{est} \\rangle = -\\sum_{i=1}^K p_i \\ln p_i -  \\sum_{i=1}^K  \\langle\\delta f_i \\rangle( 1 + \\ln p_i)  -\\sum_{i=1}^K \\frac{\\langle\\delta f_i^2\\rangle}{2p_i} \\\\\n",
      "\\ln 2 H_{est} = \\ln 2 H_{real} + \\sum_{i=1}^K \\frac{1-p_i}{2N}$$\n",
      "Finally, evalutating the sum we come to \n",
      "$$H_{est} = H_{real} -  \\frac{K-1}{N2\\ln 2}$$\n",
      "So the first order corrections in the entropy estimate are $O(1/N)$ and as we expected, they systematicallly bias the entropy to lower values.  A further calculation, keeping terms of $O(\\delta f^4)$ would yield similar corrections of $O(1/N^2)$ and so we can expect our entropy estimate to be generally of the form,\n",
      "$$H_{est} = H_{real} + \\frac{H_1}{N} + \\frac{H_2}{N^2} + ...$$.\n",
      "\n",
      "Success! Now, given a sample, we can calculate the entropy estimate for increasing subsets of the data, regress polynomially in $1/N$ and extract the asymptotic value as $N\\to \\infty$ as our entropy estimate!  Hopefully, figures showing this coming soon."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Expanding to Stochastic Processes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the example of spike trains.  In this case, we have a physical process producing a time series whose entropy we would like to estimate, but it is slightly ambiguous what we mean by entropy.  Given a time series, we could chop it up into windows of length $T$, bin these, and then estimate the entropy as in our previous example, but this entropy will depend on our arbitrarily chosen time window.\n",
      "\n",
      "We would do better to define an *entropy rate* as the number of bits produced by the source per unit time as\n",
      "$R(\\Delta\\tau) = \\frac{H(\\Delta\\tau, T)}{T}$.  But this definition is also slightly unsatisfactory.  The entropy for a finite window $T$ will contain a bulk entropy contribution that scales as the length of the window $T$ (ie is extensive), and 'surface' and other contributions that are subextensive and thus scale as inverse powers of $T$.  We can thus write the entropy of our finite window as $H(\\Delta\\tau, T) = T R(\\Delta\\tau) + C(\\Delta\\tau) + \\frac{H_1(\\Delta\\tau)}{T}...$.  This is in analogy to the free energy in a thermodynamic system where the total free energy can be decomposed into an extensive bulk term that scales as the system size $L^d$, a surface term that scales as $L^{d-1}$ and further subextensive constributions.  The surface term comes from the cutting off of the correlation length in the system and thus makes the samples appear 'more independent' than they are and should thus increase the entropy.\n",
      "\n",
      "In a time series, the surface term is constant in $T$.  Our previous definition of the rate will include this term.  In order to see the true rate, we should take the 'thermodynamic limit' of the entropy where our time window becomes very long, $\\lim_{T\\to\\infty}\\frac{H(\\Delta\\tau, T)}{T} = R(\\Delta\\tau) + \\frac{C(\\Delta\\tau)}{T} + ... = R(\\Delta\\tau)$ which defines the entropy rate.  Unfortunately, this definition is diffucult to work with practically.  For a window size $T$, there are $~2^{H(T, /Delta\\tau)} = 2^{TR(\\Delta\\tau) + ...}$ possible samples.  The exponential dependence on $T$ means long time windows will be hopelessly undersampled (and from our previous section, the entropy will be badly underestimated).  What we can do is plot our entropy estimates at various sizes of $T$ and look for a band of uniform behavior before we run into undersampling at large $T$ and the correlation length at short $T$, and use this to extrapolate to the $T\\to\\infty$ limit.\n",
      "\n",
      "To gain a little intuition for all this, consider a source that generates a random bit every $\\Delta\\tau$.  The entropy rate will be $1 \\,\\text{bit/s}$.  If we add a correlation length (ie a repetition code) so that the bit produces either 111 or 000 in each window of size $3\\Delta\\tau$ the entropy rate will be $\\frac{1}{3}\\,\\text{bit/s}$.  If we chose a time window of length $\\Delta\\tau$ we never see the dependence on their neighbors and our samples will appear to be from a source producing $1 \\,\\text{bit/s}$.  As our window becomes longer, our samples will reflect the correlation length and in the long time limit, the calculated entropy rate will settle to $\\frac{1}{3}\\,\\text{bit/s}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Entropy Bounds"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Lower Bound"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By Jensen's inequality, $H = - \\langle \\log_2 p \\rangle \\ge - \\log_2 \\langle p \\rangle = - \\log_2 P_{coinc} = H_2$. So, the Renyi entropy of order 2 gives a lower bound on the entropy.  By plotting this alongside the sample entropy rate $\\frac{H(T, \\Delta\\tau)}{T}$ an observing when the inequality is violated, we get an indicator of when our entropy estimate is valid."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Upper Bound"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For an upper bound, we have the inequality,\n",
      "$$\\frac{1}{\\Delta\\tau}\\{H(T + \\Delta\\tau, \\Delta\\tau) - H(T, \\Delta\\tau)  \\} \\ge R(\\Delta\\tau) = \\lim_{T \\to \\infty} \\frac{H(T, \\Delta\\tau)}{T} $$\n",
      "We can understand this bound simply: when we increase the sample window by $\\Delta\\tau$, the entropy must increase by an extensive contribution of $\\Delta\\tau R(\\Delta\\tau)$, but the total increase will also include contributions from cutting off the correlation length, and as our previous discussion notes, these will *increase* the uncertainty in the sample.  The change in the entropy due to lengthening the window will always be greater than the contribution from the entropy rate, even if these contributions tend to shrink as the size of the window grows.\n",
      "\n",
      "To prove this, consider a stationary stochastic process $X(t)$.  In analogy with our definition of the entropy rate above, we define the entropy of the stochastic process as\n",
      "$$H(X) = \\lim_{n\\to\\infty} \\frac{H(x_1, ...,x_n)}{n}$$\n",
      "which is again, the extensive component of the entropy.  We note that if the correlation length of the system is 0, ie $C_2(t_2 - t_1) = \\delta(t_2 - t_1)$ then $H(X) = H(x_i)$.  An alternative definition of the entropy is\n",
      "$$H'(X) = \\lim_{n \\to \\infty} H(x_n | x_{n-1},...,x_1)$$\n",
      "\n",
      "**Thm:** $H(X) = H'(X)$\n",
      "First, the limit of $H'(X)$ exists:\n",
      "\n",
      "* $H(x_n|x_{n-1}, ...,x_1)$ is positive as it is an entropy.\n",
      "* $H(x_n|x_{n-1}, ...,x_1)$ is a non-increasing function of $n$ as $H(x_n|x_{n-1}, ...,x_1) \\le H(x_n|x_{n-1}, ...,x_2) = H(x_{n-1}|x_{n-2}, ...,x_1)$ where the first inequality follows from conditioning reducing the entropy and the second follows from stationarity.  \n",
      "As $H'(X)$ is positive and non-increasing it must tend to a limit.\n",
      "\n",
      "From the chain rule for entropies,\n",
      "$$\\frac{H(x_1, ...,x_n)}{n} = \\frac{1}{n}\\left[ H(x_n|x_{n-1}, ...,x_1) + H(x_{n-1}|x_{n-2}, ...,x_1) + ... + H(x_2|x_1) + H(x_1)\\right]\\\\\n",
      "H(X) = \\lim_{n\\to\\infty} \\frac{H(x_1, ...,x_n)}{n} = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^n H(x_i|x_{i-1}, ...,x_1) = H'(X)$$\n",
      "where the last equality follows from the *Cesaro mean* property: If $a_n\\to a$ and $b_n = \\frac{1}{n}\\sum_{i=1}^n a_n$ then $b_n\\to a$. \n",
      "\n",
      "The bound is now easily proved:\n",
      "$$H(x_n, ..., x_1) - H(x_{n-1}, ..., x_1) = H(x_n | ...,x_1) \\ge \\lim_{n\\to\\infty} H(x_n |...,x_1) = H'(X) = H(X) = \\lim_{n\\to\\infty} \\frac{H(x_1, ...,x_n)}{n}$$\n",
      "If $n$ refers to a time index,\n",
      "$$H(T + \\Delta\\tau, \\Delta\\tau) - H(T, \\Delta\\tau) \\ge \\lim_{T\\to\\infty} \\frac{H(T,\\Delta\\tau)}{T/\\Delta\\tau} = \\Delta\\tau R(\\Delta\\tau)$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Removing Noise"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}